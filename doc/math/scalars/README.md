This is just an unfinished draft.

The scalars here are related to [the Infromation-Energy Theory](https://medium.com/not-science-just-a-theory/not-science-just-a-theory-information-energy-theory-c21e5a6c874a).

**h**, the nature of this scalar depends on the nature of the tag universe and concept space. To put it in more programmatic language, the h is a scalar, which measures the potential for a change. In some systems there could be mood tags, which would guide the system to make bold recommendations upon good mood and less risky recommendation upon negative mood. It could also consist of the payment history or time spent within the service by a user. NOTE: Now that I have contemplated this more in depth, it seems that **h** represents [a Baryonic system](https://medium.com/project-obhave/obhave-the-baryon-number-of-the-content-grid-as-iteration-reducer-7ef61c231079) and it could simply be the amount of content items included within the recommendation set.

**m**, the abstraction of the Social Networks (size and topology) as scale-free macro or micro level network, which is shaped by domain specific rules, which compare the m of the subject to the m of the actor (a recommendation by a friend or the system or a tag or a concept within the system). NOTE: **m** is also [related to the Baryonic system](https://medium.com/project-obhave/obhave-the-baryon-number-of-the-content-grid-as-iteration-reducer-7ef61c231079), which is formed from the different weights of importance between recommendation sets. More massive recommendations are likely to submerge the less massive recommendations, however upon such interaction particular "energy states" are changed [as in Hamiltonian mechanics used within quantum mechanics](https://en.wikipedia.org/wiki/Hamiltonian_(quantum_mechanics)).

d, The Effort of Influence. This is probably related to the learning rate and computational iterations and priorities. The machine learning problem is to reduce d to minimum or maximize the efficiency. For example, the User Interface should consist of different efforts of influence, like content priorities. If Influence Force and the Mood of the user would create a probability combination, which would at the present moment allow risky recommendation with less effort of influence, it should be chosen. When the recommendation environment requires more effort than usually, it should not be selected. This is related to the Nash Equilibrium and auction theories of maximizing benefits. This is also exceptional to traditional machine learning algorithms, as it is a feature, which is only beneficial to parallel decentralized almost real-time learning; which is only useful feature for a machine learning recommendation engine targeted as a web-application back-end. Traditional machine learning tries to solve complex problems through converge of probable errors with superhuman processing power.

cos(theta), forms a multidimensional Cost Factor of Influence, which works as Homomorphic Filter matrix between the nature of the F vector. Even though the recommendation algorithms and OBHave! instances compete within the same system, their learning from others should not be direct, but rather relativistic.

c, The Euclidean Distance of the network, which will create the global connection between all entities within certain levels of abstraction. A probability of recommendation would be 100% within a network of start topology of homogeneous entities. This should form basis for a Von Neuman Entropy of recommendation structures, which can be utilized to sub-optimize result sets in order to reach business goals more effectively within finite computing resources.

hv (Planck), The Relationship Between the history data and the most current learning set. This should represent the minimum energy required for learning to happen and maintain diversity of the recommendations. There should exist a realtion to c.

In the OBHave! [Bayesian Front-End Game of Recommendations](https://medium.com/project-obhave/obhave-bayesian-front-end-game-of-user-stereotypes-902dd718c0a3), the content grid has predefined popularity threshold, which defines that the recommendation set should contain at least one popular content item, and at least one content item that is enough less popular in order to enable diversity of the learning. The popular item is related to the c and could possess energy or v of 90% of the c; the less popular recommendations would have logarithmic classifications; v = 9% * c, v = 0.9% * c etc.

I will update this later when I know more...
